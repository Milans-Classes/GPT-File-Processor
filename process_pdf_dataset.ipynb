{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62af9f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303b3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pypdf import PdfReader\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. SETUP\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "YOUR_OPENAI_API_KEY = \"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\"\n",
    "client = OpenAI(api_key=YOUR_OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da87a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DEFINE TOOLS\n",
    "# ---------------------------------------------------------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Reads a PDF and returns the text string.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac9d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(document_text, question):\n",
    "    \"\"\"Sends text + question to LLM.\"\"\"\n",
    "    if not document_text or len(str(document_text)) < 50:\n",
    "        return \"Skipped (Text too short or empty)\"\n",
    "\n",
    "    try:\n",
    "        # Truncate to ~15k chars to save costs/tokens if docs are huge\n",
    "        truncated_text = str(document_text)[:15000] \n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Based on this text: {truncated_text}\\n\\nAnswer: {question}\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e5ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/milanmiric/Downloads\")\n",
    "\n",
    "path = \"./2601.20856v1-2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3b409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The specific question you want to answer for ALL documents\n",
    "QUESTION = \"What is the focus of this research paper?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cc72fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82bcc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SokoBench: Evaluating Long-Horizon Planning and Reason-\\ning in Large Language Models\\nSebastiano Montis.monti@ipazia.com Ipazia SpA, Italy\\nCarlo Nicolinic.nicolini@ipazia.com Ipazia SpA, Italy\\nGiovanni Pellegrinig.pellegrini@ipazia.com Ipazia SpA, Italy\\nJacopo Staianojacopo.staiano@unitn.it University of Trento, Italy\\nBruno Leprilepri@fbk.eu Fondazione Bruno Kessler and Ipazia SpA, Italy\\nAbstract\\nAlthough the capabilities of large language models have been increasingly tested on complex\\nreasoning tasks, their long-horizon planning abilities have not yet been extensively investi-\\ngated. In this work, we provide a systematic assessment of the planning and long-horizon\\nreasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a\\nnovel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon\\nplanning from state persistence. Our findings reveal a consistent degradation in planning\\nperformance when more than 25 moves are required to reach the solution, suggesting a\\nfundamental constraint on forward planning capacity. We show that equipping LRMs with\\nPlanning Domain Definition Language (PDDL) parsing, validation, and solving tools allows\\nfor modest improvements, suggesting inherent architectural limitations which might not be\\novercome by test-time scaling approaches alone.\\n1 Introduction\\nAutomated Planning, i.e. the task of generating sequences of actions to achieve a goal, is a well-studied\\nproblem in the field of Artificial Intelligence (AI) (Ghallab et al., 2016), since it requires AI systems to\\nexhibit cognitive abilities such as reasoning, understanding, and efficient state space search (Wei et al.,\\n2025). To this end, automated planning literature has focused on the use of formal languages, such as the\\nPlanning Domain Definition Language (PDDL) (McDermott et al., 1998; Russell & Norvig, 2021; Haslum\\net al., 2019)), and of tree-search strategies or specific heuristics to find optimal solutions (Bonet & Geffner,\\n2001). Large Language Models (LLMs) and, in particular, Large Reasoning Models (LRMs) i.e., LLMs\\ntrained to produce so-calledreasoning tracesresembling structured thought processes, have demonstrated\\nimpressive capabilities in natural language understanding, knowledge retrieval and multi-modal pattern\\nrecognition (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025). However, recent studies highlighted the\\nlimitations of such models when applied to planning tasks (Valmeekam et al., 2023b; Shojaee et al., 2025).\\nFor instance, internal reasoning processes have been shown to resemble a form of wandering through the\\nsolution space rather than a systematic exploration (Lu et al., 2025). This distinction becomes particularly\\nimportant for problems that require maintaining sequential state information, such as spatial exploration in\\nconstrained environments. In these settings, effective tracking of working memory is necessary to infer the\\nagent’s latent previous state (Zhang et al., 2024).\\nIn this work, we investigate the long-horizon planning abilities of LRMs using a highly simplified variant of\\nthe Sokoban puzzle (Culberson, 1998). Rather than increasing spatial complexity, wedeliberately minimize\\nthe structural complexityof the environment while preserving the long-horizon nature of the task by creating\\n1\\narXiv:2601.20856v1  [cs.AI]  28 Jan 2026\\nexamples with the lowest possible branching factor compatible with solvability: a single movable block placed\\nwithin a linear corridor with tightly controlled geometry.\\nThis setting allows us to isolate long-horizon planning from state persistence: models are required to produce\\ncomplete solution sequences without external memory, intermediate feedback, or state validation, relying\\nsolely on internal state representations to track the evolving environment. We therefore investigate to\\nwhat extent LRMs can sustain coherent planning over long (but simple) action sequences and whether even\\nminimal reasoning branching in otherwise trivial Sokoban instances is sufficient to induce planning failures.\\nConcretely, we examine whether current LRMs can reliably solve linear-corridor Sokoban puzzles with min-\\nimal possible branching and identify the point at which increases in horizon length lead to catastrophic\\nbreakdowns in action validity, despite the simplicity of the underlying environment. As we will show these\\nminimal sub-problems which are trivial to humans (Jarušek & Pelánek, 2010), are still challenging for Large\\nReasoning Models as shown by other preliminary studies involving spatial intelligence (Cai et al., 2025). We\\nposit this as a systemic deficiency in long-term action representation and sequential logic, and in spatial\\nreasoning and thus as an important limitation of current LRMs that is not yet fully understood.\\n2 Related Work\\n2.1 Benchmarks for LLM Planning\\nAs mentioned above, planning requires LLMs to blend logical, numerical, and spatial reasoning with long-\\nhorizon strategic adaptation, rather than just relying on pattern matching or memorization. Classical plan-\\nning domains expressed in or derived from the Planning Domain Definition Language (PDDL (Fox & Long,\\n2003)), such as BlocksWorld (Slaney & Thiébaux, 2001), Towers of Hanoi and similar tasks (Pallagani et al.,\\n2023), remain a common benchmark choice, though earlier attempts date back to the pre-ChatGPT era\\n(Silver et al., 2022). Test suites like PlanBench (Valmeekam et al., 2022) introduced structured, domain-\\nagnostic evaluations inspired by classical planning (Ghallab et al., 2016), including plan generation (Oswald\\net al., 2024; Valmeekam et al., 2025; La Malfa et al., 2025) and optimality (Valmeekam et al., 2022; Zhai\\net al., 2025; Valmeekam et al., 2023a).\\nIn another line of work, planning is evaluated withinagenticor workflow-based frameworks, where LLMs\\nare required to decompose goals into multiple sub-plans (Meyerson et al., 2025; Zhang et al., 2025; La Malfa\\net al., 2025). The results in these settings are encouraging though highly cost intensive. Importantly, when\\nnot equipped with external tools or made part of larger workflows (e.g., enabling stateful tracking (Hu\\net al., 2025b)), innate planning abilities remain still weak (Schepanowski & Ling, 2025). Even the latest\\nfoundational models are found to consistently fail in delivering correct sequences of actions (in any format\\nor language) due to two primary deficits: weak internal state representations leading to invalid moves and\\nmisleading heuristic search resulting in loops or early termination, as shown in the textual game “8-puzzle”\\nin Schepanowski & Ling (2025). Moreover, efficacy of different prompting techniques is model-dependent in\\na non-predictable way (Schepanowski & Ling, 2025; Deng et al., 2025).\\nOther works have systematically investigated the performances of LLMs in playing textual games withgym-\\nstyle APIs (Brockman et al., 2016; Hu et al., 2025a). Beyond structured puzzles, community-driven and\\ninformal game-oriented benchmarks like word-game bench (Stojanovski, 2024) and nonogram logic puzzles\\n(Berend et al., 2014; Kleine, 2026) with multi-difficulty instances have been devised to measure how well\\nmodels plan under both explicit and implicit constraints, track environment states, and adapt over multiple\\nturns. The varying depth of planning ability required helps to reveal how performance scales with complexity\\nand structure.\\nIn general, existing benchmarks using specific planning languages and/or internal reasoning traces expressed\\nin natural language show that LLMs exhibit limited planning abilities in various domains (Kambhampati\\net al., 2024), especially as the complexity and horizon length of the problems increase. This gap motivates\\nthe development of new benchmarks tailored to planning and solving structured textual puzzles with LLMs.\\n2\\n2.2 Sokoban as a Benchmark for Planning\\nThe Sokoban puzzle involves spatial planning in a highly constrained environment. Solvable Sokoban maps\\ncan be generated efficiently (Murase et al., 1996), and the environment is fully controllable and deterministic.\\nThese properties enable rigorous evaluation using exact solvers and verifiers, as well as metrics such as search\\ndepth and solution time (Jarušek & Pelánek, 2010; Shoham & Schaeffer, 2020). Unlike puzzles such as the\\nTower of Hanoi, which can be solved by repeating a simple pattern for larger instances, Sokoban offers no\\nshortcuts. Each map is unique, and moving a single box can block or open paths in ways that prevent\\na one-size-fits-all solution. As a result, Sokoban is considered a good benchmark for evaluating planning\\nabilities in the 2023 edition of the International Planning Competition (Taitler et al., 2024).\\nRecently, recurrent neural networks (non LLM-based) trained over multiple examples of Sokoban puzzles\\nhave obtained state of the art performance (Jolicoeur-Martineau, 2025; Taufeeque et al., 2024). However,\\nLLMs are found to perform poorly, struggling even with simple maps and correctly solving only a small\\nfraction of instances: Valmeekam et al. (2025) report success rates of just about 10–12% when using the\\nOpenAIo1-previewmodel directly. In contrast, substantially higher success rates are achieved in an LLM-\\nModulo setting, where the same model is used to generate plans that are then executed by an external\\nplanner, yielding approximately 43% solved instances foro1-preview(and about 10% foro1-mini), albeit\\nat significantly higher computational cost.\\nMost prior work on textual puzzle solving and planning with LLMs has emphasized high-level notions such\\nas search depth, branching factor, or overall puzzle complexity. Much less attention has been paid to the\\nrole of simpler, low-level operations that these tasks implicitly rely on. Evidence from seemingly trivial\\nproblems suggests that LLM failures do not always stem from complexity itself, but from how basic rea-\\nsoning steps are elicited. A well-known example is the character-counting question “how many r’s are in\\nstrawberry?” (Karpathy, 2024), which has sparked debate over whether LLM errors are caused by tokeniza-\\ntion or deeper representational limits (Shin & Kaneko, 2024). The work by Xu & Ma (2025) revisits this\\nissue through a careful empirical study, showing that LLMs are in fact capable of performing these simple\\nsymbolic operations, but often fail unless prompted to reason explicitly. Character-level benchmarks, such\\nas CharBench (Uzan & Pinter, 2025), shows that modern LLMs struggle with simple character counting\\nand positioning tasks not because tokenization fully explains these errors, but because intrinsic properties\\nlike word length and actual character count have a stronger influence on performance, indicating that basic\\nsymbolic operations are not reliably deployed unless the model is guided to engage them explicitly.\\nPut together, these observations point to a broader interpretation of failures in spatial planning and puzzle\\ngames, suggesting that they may arise from missing or weak activation of basic operations, rather than from\\nthe inherent difficulty of the planning problem.\\n3 Methods\\n3.1 Sokoban game\\nFigure 1 shows an example of a Sokoban puzzle and the game’s central mechanic: the player controls\\na sprite that pushes boxes within a two-dimensional spatially constrained environment with the goal to\\nposition them onto predefined locations. Despite its apparent simplicity, Sokoban is a NP-hard and PSPACE-\\ncomplete problem (Culberson, 1998), positioning it as a canonical domain for symbolic and hierarchical\\nplanning. Apart from the pictorial representation, Sokoban maps can be encoded using an ASCII-based\\nsymbolic representation as expressed in Table 1. Sequences of main character actions are typically encoded\\nin LURD format (left,up,right,down), with lowercase letters indicating simple moves, and uppercase letters\\nindicating box pushes. Although moves and pushes have distinct notations in classical Sokoban planning,\\nin our experiments we restrict only to comma-separated uppercase letters. This representation does not\\ncompromise the information content of the solutions and simplifies the output format for language models,\\navoiding potential mistakes.\\n3\\nFigure 1: Example of a Sokoban puzzle. All boxes\\nmust be pushed onto goal positions. A solution to\\nthis problem in compressed notation is:1↑,4←,\\n1↓,1→,1↓,4→, resulting in the LURD notation\\nu,l,l,l,l,D,r,d,r,r,r,R.\\nEquivalent ASCII format\\n# # # # # # # # # #\\n# #\\n# $ # @ #\\n# . $ . #\\n# # # # # # # # # #\\nGame Map Element ASCII Symbol\\nSokoban\\nPlayer @\\nPlayer on Goal +\\nBox $\\nBox on Goal *\\nGoal .\\nWall Brick #\\nTable 1: ASCII notation of the elements of\\nSokoban maps. Empty areas are encoded as\\nspace (␣).\\n3.2 Dataset\\nWe generated a dataset consisting of narrow, corridor-like maps, i.e. maps of widthℓand height1. Each\\nmap contains the same set of elements: one player, one box, and one goal. The maps share the same initial\\nconfiguration in which the goal is positioned at one end of the map, the player at the opposite end, and\\nthe box placed in between the two, so that all elements lie along the same row or column. This choice is\\nmotivated by its simplicity: the corridor lengthℓis the only map parameter and it serves as a proxy for\\nmap difficulty. Hence, with just one degree of freedom to account for, we overcome the problem of defining\\ncomplex measures for solution difficulty: the longer the map, the harder the task.\\nIn our benchmark, we consider map lengthsℓranging from 5 to 100 in increments of 5. For each map, we\\ngenerate four augmented variants corresponding to rotations of90◦,180 ◦, and270 ◦, as well as the original\\n(unrotated) orientation. This augmentation strategy reduces the risk of querying the model with data that\\nmay have been encountered during pretraining and enables analysis of whether models exhibit orientation-\\ndependent performance. In total, the evaluation set comprises 80 distinct maps, spanning 20 values ofℓwith\\nfour orientations each. We publicly release our dataset athttps://huggingface.co/datasets/Linello/\\nsokobanlevels.\\n3.3 Experimental Setup\\nWe employ both open and closed weights model, specificallyDeepSeek R1(Guo et al., 2025),GPT-5and\\nGPT-oss 120B(OpenAI, 2026; 2025). They are allreasoning models, i.e., they are configured to generate\\nan explicit reasoning trace prior to emitting the final answer to the user query. For GPT models, we don’t\\nchange the default temperature neither the default reasoning effort (set to medium). Instead we cap the\\nmaximum number of completion tokens (including both reasoning and final answer tokens) at32,768. All\\ninference calls are routed through OpenRouter,1 with the inference provider consistently set to DeepInfra.2\\nIn light of both computational and financial resource constraints, we limit our empirical analysis to these\\ntwo primary model families.\\n3.3.1 1-shot Inference\\nIn the first experimental setup, we test the ability of the selected LRMs to solve simple Sokoban puzzles when\\nprovided only with the instructions, the mapping of characters as in Table 1 and a single demonstration.\\nUnder this setup, thus, models are by design limited to use exclusively their internal state representations\\n1https://openrouter.ai/\\n2https://deepinfra.com/\\n4\\nOutput\\nInput\\n LLM-Modulo\\nLLM or LRM\\nPDDL Tools\\nDomain Parser\\nProblem Parser\\nProblem Solver\\nPDDL Domain\\nMCP\\nMCP\\nMCP\\nPDDL Problem;\\nPDDL Plan;\\nSolver errors.\\n(a)\\nOutput Example\\n<problem>(define (problem sokoban-prob1)    (:domain sokoban)    (:objects x1 x2 x3 y1 y2 y3 y4 y5 y6 y7)\\n    (:init        ; walls row1\\n        (wall x1 y1) (wall x1 y2) (wall x1 y3) ...        ...\\n        ; box and player        (box x2 y4)\\n        (at x2 y6)\\n        ...\\n    )    (:goal (and (box x2 y2))))</problem>\\n<plan>move-left(x2, y6, y5)push-left(x2, y5, y4, y3)\\npush-left(x2, y4, y3, y2)</plan><solver>True</solver> (b)\\nFigure 2: Panel (a) represents a simple schema of our LLM-modulo pipeline. The detailed input prompts\\nare collected in Appendix B.1, while an example of output is shown in Panel (b).\\nto solve Sokoban puzzles of varying solution lengths. The prompts used for all models are described in\\nAppendix A.\\n3.3.2 LLM-Modulo\\nIn the second experimental setup, we investigated how Sokoban puzzle–solving performance can be enhanced\\nwhen LRMs are provided with access to external planning solvers, within an LLM-modulo framework analo-\\ngous to that of Valmeekam et al. (2023a). To this end, we prompted the models to generate specific instances\\nof planning problems while providing them with a pre-existing, human-authored and verified PDDL domain\\n(Appendix B.2). In this setup, the model is responsible solely for formulating the PDDL problem, which is\\nthen processed through an agentic pipeline. This workflow utilizes a domain parser to instantiate the formal\\nworld representation and a dedicated problem parser that acts as a validator, informing the model whether\\nthe generated problem is syntactically and semantically well-formed. Finally, the pipeline provides access\\nto specialized PDDL planners such asFast-DownwardorPyperPlan, integrated via theUnified Planning\\nlibrary (Micheli et al., 2025; Alkhazraji et al., 2020; Helmert, 2006) to solve the problem and get the optimal\\nplan. All the tools were wrapped and made accessible to the LRMs via a custom Model Context Protocol\\nlibrary (Anthropic, 2024) implemented withFastMCPlibrary (Lowin, 2024). The design of our architecture\\nis shown in Figure 2.\\nThe planner tool produces a variety of diagnostic and informational messages that are provided back to the\\nmodel, including error reports, timing information, and the complete raw response. This raw response can be\\nfurther processed to extract the LURD solution in cases where the problem is successfully solved. In failure\\nscenarios, the tool returns the encountered errors in natural language to the LRM. Errors or warnings are\\ngenerated in situations such as logically inconsistent or unsatisfiable problems, invalid or inappropriate initial\\nconditions, or when the solver exceeds the maximum allotted execution time (60 seconds). The agentic loop\\nends either with a valid plan or with a message to the final user explaining that, after three failed attempts\\n(which may include having the LRM reformulate the PDDL problem), the agent could not find a satisfactory\\nsolution.\\nThe LRM-modulo pipeline is considerably slower than the reasoning-only one. It took an average of 75\\nminutes usingGPT-5-minion an AWSt3.xlargeinstance (4 CPUs at 3.1 GHz, 16 GB RAM) to collect\\nthe points shown in Figure 6a. The prompts being used for the experiments are described in Appendix B.\\n5\\n3.4 Evaluation\\nA solution to a Sokoban instance is defined as a sequence of actions that transforms the system from its\\ninitial configuration to the final state, where all boxes are correctly placed on goal positions. Multiple valid\\nsolutions may exist for the same map, however we restrict the evaluation only to optimal solutions, i.e.,\\nsequences that achieve the goal with the minimum possible number of moves. The intrinsic simplicity of our\\nsetting makes optimality the natural criterion. Clearly, the one-dimensional layout of the map allows only\\nfor a unique optimal solution.\\nAccuracy:Given a map of lengthℓ, we define the accuracy in Eq. 1 as the expectation, over all repetitions\\nand rotationsNof the indicator of exact string equality (via Iverson brackets) between the predicted action\\nsequence ˆx(ℓ)and the ground-truth sequencex(ℓ), i.e., the fraction of trials in which the two character strings\\nare identical:\\nAccuracy(ℓ) = 1\\nN\\nN∑\\nn=1\\n[ˆx(ℓ) =x (ℓ)].(1)\\nHereNis the product of the total number of trialsnt and the number of map rotationsnr = 4. Increasingn t\\nmitigates the intrinsic non-determinism of the obtained solutions, by sampling at multiple seeds. In the LRM\\nexperiments, we set the number of repetitionsnt to 8. Conversely, in the LRM-modulo experiments, the\\nsubstantially higher computational and monetary costs imposed stricter constraints. We therefore reduced\\nthe number of repetitionsnt to 4.\\nPrefix accuracy:Alongside the standard accuracy metric, we define Prefix Accuracy (Eq. 2) to provide\\na more granular evaluation of model performance. This metric calculates the average proportion of correct\\nsymbols generated by comparing the predicted and true plans’ strings element-wise:\\nPrefixAccuracy(ℓ) = 1\\nN\\nN∑\\nn=1\\n[m(n)≤ℓ]\\nℓ\\nm(n)\\n∑\\ni=1\\n[ˆx(n)\\ni =x (n)\\ni ],(2)\\nwherem (n) is the length of the predicted planˆx(n). Unlike the hard matching of the standard accuracy\\nmetric, prefix-accuracy is more optimistic, rewarding the model for correct partial trajectories even if it stops\\nprematurely. However, it remains strictly penalized for overshooting: if the predicted lengthm(n) exceeds\\nthe ground-truth lengthℓ, the score for that trial is0. For instance, a predictionˆx(n) = (l, l, l)against\\na ground truthx(n) = (l, l, l, l)yields a score of3/4, whereas any prediction exceeding length 4 results\\nin a score of0.\\nManhattan Distance:While string-based metrics evaluate the symbolic fidelity of the action sequence,\\nthey do not account for the spatial proximity of the agent to the objective. We therefore use the Manhattan\\nDistance (Eq. 3) to measure theL1 distance between the agent’s terminal position and the goal, independent\\nof sequence semantics or environmental obstacles.\\nD(ℓ) = 1\\nN\\nN∑\\nn=1\\n(\\n|x(n)\\nfinal−x(n)\\ngoal|+|y(n)\\nfinal−y(n)\\ngoal|\\n)\\n(3)\\nHere,(x (n)\\nfinal, y(n)\\nfinal)represents the agent’s coordinates after executing all moves in the predicted sequence\\nˆx(n), starting from the origin(0,0). The goal coordinates(x(n)\\ngoal, y(n)\\ngoal)are always at a fixed distanceℓfrom\\nthe origin, specifically(±ℓ,0)for0◦/180◦ rotations and(0,±ℓ)for90◦/270◦ rotations.\\nThe primary motivation for this metric is todistinguish between “near-misses” and total navigational failures.\\nBy measuring spatial displacement, we can quantify whether a model that failed the exact string match\\nnonetheless moved in the correct direction or reached the vicinity of the goal. This provides a soft failure\\nsignal that string-based metrics like Accuracy or Prefix Accuracy cannot capture.\\n6\\n4 Results\\n4.1 1-shot Inference\\nFigure 3 summarizes the results in terms of accuracy and total token usage. The plot on the left of Figure 3\\nshows the accuracy as a function of the corridor length,ℓ, for all tested models. Similarly to Shojaee et al.\\n(2025), our results show approximately three regions in which the models behave according to different\\nregimes: an easier region where corridor lengths are short, characterized by higher accuracy; an intermediate\\nregion characterized by a rapid decrease in accuracy as the length of the corridors increases and a harder\\nregion in which the models completely fail to return a correct plan. These regions are specific to each model.\\nCrucially, corridors aredeep but narrowproblems: many sequential steps (depthd∼ℓ) with minimal\\nbranching. In such settings, a small per-step probabilitypw of miscounting the size of the map compounds\\nexponentially, yielding success probability∼(1−pw)ℓ. This may explain the three-region performance\\ncurve we observe: short corridors tolerate occasional drift, producing a plateau of acceptable accuracy;\\nintermediate lengths mark the onset of exponential decay, while long corridors see near-total collapse as\\ncumulative errors dominate. We thus believe that the main reason LRMs cannot correctly plan in longer\\ncorridors is mainly due to internal counting representation. It was indeed shown in McCoy et al. (2024) that\\nwhen asked to count individual characters, LLMs perform better with common characters than uncommon\\nones (like#). This counting failure can be interpreted through the lens of Lu et al. (2025) “wandering\\nvs systematic exploration” framework: maintaining an accurate count over many positions is equivalent to\\nmaintaining correct state representations across a chain of transitions.\\nAs an observation, we report thatGPT-5-minidisplays an anomalous accuracy peak aroundℓ= 50which\\nis however hardly explained by the model above. We believe this effect is likely due to memorization, but\\nwithout access to internal states models this remains an hypothesis.\\nFigure 3 shows the number of output tokens as a function of the corridor length,ℓ, filtering only for correctly\\nsolved Sokoban problems. Linear regression analysis reveals that for each model, the number of output\\ntokens for correctly predicted problems increases with the length of the corridor. We don’t observe the\\ncounterintuitive scaling mentioned by Shojaee et al. (2025) with models declining the request to do very long\\nreasoning to solve complex problems. Instead, we report the reasoning effort increasing almost linearly with\\nproblem complexity, with none of the three models declining our request early.\\nIn the linear regression analysis above, however only a small fraction of the variance is explained due to the\\nnoise of the measurements. This trend is observed only in the region whereℓ <50, since for larger corridor’s\\nlengths the number of correct predictions decreases significantly for all tested models. Main parameters of\\nthe linear regression fit are collected in Table 2.\\nModel Slope R2\\nDeepSeek R1 51.1 0.35\\nGPT-5-mini 29.8 0.62\\nGPT-oss 120B 39.4 0.40\\nCorrect Answers\\nModel Slope R2\\nDeepSeek R1 86.3 0.25\\nGPT-5-mini 55.2 0.14\\nGPT-oss 120B 85.9 0.12\\nWrong Answers\\nTable 2: Fit parameters associated to the linear regressions performed on Figure 4 (see below).\\nIn Figure 4 we further analyze the number of emitted tokens as a function of the corridor length parameterℓ,\\nconsideringbothcorrectandincorrectanswers. UnlikeShojaeeetal.(2025)whichobservedacounterintuitive\\nreduction in the reasoning effort for problems above a certain threshold of difficulty, we observe a steady\\nincrease in the number of output tokens. What we found shows that the difficulty of a problem is not\\ncharacterized by the decrease in the reasoning effort, but instead by the substantially higher variability in\\ntoken counts of incorrect answers compared to correct ones. This suggests that when the model diverges\\nfrom the correct reasoning trajectory, it can fail in multiple ways, whereas successful completions remain\\nmore concise and consistent, likely an effect of inductive bias of Group Reinforcement Policy Optimization\\n7\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length \\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Average Accuracy\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length \\n0\\n2000\\n4000\\n6000\\n8000\\n10000Reasoning Tokens\\ndeepseek-r1-0528 gpt-5-mini gpt-oss-120b\\nFigure 3: Accuracy and number of reasoning tokens for LRM experiment. Left: average accuracy. Error\\nbars are computed as the 5th and 95th percentile of responses. Right: scaling behaviour of reasoning length\\nagainst corridor length filtered for correct solutions only.\\n(GRPO) post-training, where concise reasoning traces are preferred to lengthy ones (Sui et al., 2025). To\\nquantify this effect, we fit a robust regression of completion tokens against corridor’s length for each model.\\nBoth slope and intercept appear model-specific: more efficient models, such asGPT-5-mini, show lower\\nslopes and reduced variability across both correct and incorrect responses. Another relevant distinction can\\nbe made, highlighting differences in models’ calibration.DeepSeek-R1andGPT-5-minidisplay similar slopes\\nand intercepts between correct and incorrect predictions,GPT-oss-120Binstead reflect large differences in\\nthe regression parameters. A recurrent behavior is that for longer corridors, LRMs often reach the maximum\\nallowed number of output tokens. After a qualitative inspection of the reasoning traces, we observed that\\nthe main reason this happens is that the models get stuck in repeating the same action or reasoning frame\\nover and over until they reach the token limit. We report the reasoning traces for the interested user at\\nhttps://anonymous.4open.science/r/sokoban_traces/\\nThisrepetitiveloopingbehaviorexemplifieswhatLuetal. (Luetal.,2025)classifyasunnecessary exploration\\nand failure to maintain a visited-state set. In a systematic search, an agent would track which configurations\\n(or reasoning states) have already been explored and avoid revisiting them. The token-limit exhaustion we\\nobserve suggests that LRMs lack such memory: they repeatedly propose the same moves or reasoning steps\\nwithout recognizing the cycle. This is evidence ofwanderingrather than systematic planning: the model\\nexplores aimlessly rather than pruning redundant paths. In a corridor setting, where the state space is\\nessentially linear, even a simple mental tape of visited positions would suffice to prevent loops; the inability\\nto maintain it indicates a fundamental deficit in structured state tracking.\\nIn Figure 5 we analyze our data from the point of view of prefix accuracy and Manhattan distance. The\\nmetrics show a decreasing trend for all models that is similar to that represented in Figure 3. Some patterns,\\nlike the peak atℓ= 50forGPT-5-miniand the increase in accuracy around the central region forDeepSeek\\nR1, are further accentuated. This highlights that the main source of errors in most Sokoban problems is\\nrelated to counting mistakes. In terms of Manhattan distance, the optimal solution would have distance one\\nas the player and the goal are separated by the box. However, as observed sometimes the player is positioned\\nexactly on the goal, thus ignoring the spatial constraints of the problem.\\nThese violations, where the predicted sequence places the player on the goal position despite walls and box,\\nare instances of what Lu et al. (2025) termsinvalid exploration. In a valid state-transition graph, certain\\n8\\n20 40 60 80 100\\nCorridor Length \\n0\\n2500\\n5000\\n7500\\n10000\\n12500\\n15000Completion Tokens\\nmodel = deepseek-r1-0528\\ncorrect\\nFalse\\nTrue\\n20 40 60 80 100\\nCorridor Length \\nmodel = gpt-5-mini\\n20 40 60 80 100\\nCorridor Length \\nmodel = gpt-oss-120b\\nFigure 4: Number of completion tokens produced by each model as a function of corridor length. Separate\\nlinear regressions are fitted for correct and incorrect responses, with outliers excluded. A small jitter is added\\nto the x-axis to improve visualization.\\nmoves (e.g., walking through walls, teleporting over boxes) are inadmissible. When a model proposes such\\ntransitions, it demonstrates that its internal representation does not faithfully track the game’s physics and\\nits constraints. LLMs hallucinate states unreachable under the true transition rules, producing reasoning\\ntraces that are syntactically plausible but structurally incoherent for the problem. The fact that even\\nadvancedreasoningmodelsexhibittheseerrorsunderscoresacorelimitation: withoutexplicitstate-transition\\nverification, test-time scaling cannot guarantee adherence to problem constraints and rules.\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Prefix Accuracy\\n(a)\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length\\n100\\n101\\n102\\n103\\n104\\nManhattan Distance\\n (b)\\nFigure 5: Other useful metrics represented as functions of the corridor’s length. Panel (a) represents prefix\\naccuracy, computed as described in Equation 2. Panel (b) represents Manhattan distance, computed as in\\nEquation 3. Models’ colors are the same as in Figure 3.\\n4.2 LLM-Modulo\\nFigure 6 shows the main results of the LRM-Modulo approach based on classical PDDL planning tools (do-\\nmain, problem parsers and a problem solver). Unfortunately, preliminary experiments showed that not many\\nmodels are both affordable in terms of costs and effectiveness in tool-use tasks. Typical failures we encoun-\\ntered in testing models likeDeepSeek R1,Gemini-2.5-Flash-Preview, andClaude-3.5-Haikuinclude:\\nlimited capability to interact with tools, difficulty to generate coherent PDDL problems even for simpler\\nSokoban problems, and inability to stop calling tools after a given number of attempts.GPT-5-miniresulted\\n9\\nas the only model among the tested ones that could generate accurate PDDL problems and interact with\\ntools while following prompt instructions. Due to the higher costs of the experiments in the LMR-Modulo\\nsetting we limited the experiment’s repetitions per corridor rotationnt to four. Nonetheless,GPT-5-mini\\nexhibits high stability in the accuracy and in the number of reasoning tokens, allowing to maintain a valid\\nevaluation even with a lower number of repetitions.\\nIn Figure 6a, the absence of sharp peaks and the slower descending trend highlights a more regular accuracy\\nbehavior compared to that shown in Figure 3 for LRMs alone. However, a higher variability is observed\\nand the main reason is due to non-homogeneous performances across experimental trials and map rotations\\nfor a fixed corridor length. Visual inspection of the results reveals a significant imbalance between accuracy\\nin vertical and horizontal corridors (Figure 8, Appendix C) showing that, also in LRM-modulo setting,\\nmodels struggle to solve vertical corridors. At the same time, a detailed analysis of the source of these\\nerrors indicates two main causes of failure. One occurs when there are syntax errors in the generated PDDL\\nproblems, producing error messages when calling the solver tool. The other occurs when generated PDDL\\nproblems are syntactically correct but do not represent the actual Sokoban problem. In our data, first-type\\nerrors just occur 7 times out of all four trials of the 80 corridor configurations, meaning that in the large\\nmajority of cases the solver tool compiles correctly and produces a valid solution. The charts depicting\\nthe prefix accuracy and the Manhattan distance, represented in Figure 7, confirm that in many cases the\\ngenerated PDDL representation of the Sokoban problems leads to solutions in which the player, although\\nmoving in the right direction, does not reach the number of moves required to push the box towards the goal\\nposition.\\nBy utilizing an expert-validated PDDL domain and a solver strictly governed by logical constraints, we have\\neffectively eliminated the risk of invalid transitions. Hence, the primary challenge for these models lies in\\nmaintaining a consistent internal representation of the spatial environment. Evidence suggests this difficulty\\nmay stem from a fundamental limitation in the models’ ability to precisely quantify the dimensions of the\\nmap.\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length \\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Average Accuracy\\n(a)\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length \\n10000\\n20000\\n30000\\n40000\\n50000\\n60000\\n70000\\n80000\\n90000\\n100000T otal T okens\\nmodel = gpt-5-mini\\ncorrect\\nFalse\\nTrue (b)\\nFigure 6: Results of the LLM-Modulo approach forGPT-5-mini. Panel (a) represents the average accuracy\\n(Eq. 1). Panel (b) shows the counts of total tokens, for correct (green) and incorrect (red) predictions,\\ntogether with separate robust linear regressions. Error ribbons are computed as 5 and 95 percentiles. A\\nsmall jitter is added to the x-axis to improve visualization.\\n5 Conclusions\\nThe assessment of the long-horizon planning capacities of language models is both required and attainable.\\nAdhering to the principle of beginning with simplistic settings before advancing to more intricate ones, we\\n10\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length \\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Prefix Accuracy\\n(a)\\n0 10 20 30 40 50 60 70 80 90 100\\nCorridor Length \\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70Manhattan Distance\\n (b)\\nFigure 7: Additional metrics forGPT-5-miniin the LLM-modulo framework. Panel (a) represents prefix\\naccuracy (Eq. 2). Panel (b) shows Manhattan distance (Eq. 3).\\nproposeutilizingasimplifiedversionofSokobanasacontrolledenvironmenttoevaluateplanningcapabilities.\\nOur observations, in agreement with prior research, suggest that long planning abilities of LLMs may not\\nonly be related to problem complexity but from lack of more elementary initial abilities like counting.\\nWeobservethatevenadvancedreasoningmodelsstruggletosolveSokobaninstancesthatrequireanticipating\\nthegoalstatemorethan25–30movesahead. Wediscussedseveralpossiblecausesforthislimitedperformance\\nin the limitations section, including the absence of textual cues and the inability to reliably store intermediate\\nstates within model hidden representations.\\nEquipping language models with a PDDL parser, validator, and solver slightly improves planning capabilities\\non average, but not enough to overcome the lack of inherent spatial grounding. We found that the basic,\\ninitial inability to track counts remains a persistent bottleneck. This issue surfaces even in LLM modulo\\nsettings where external symbolic engines are used, proving that offloading logic to a solver cannot fully fix\\na model that cannot faithfully represent space and constraints.\\nMore broadly, our observations align with recent characterizations of reasoning models as “wanderers” rather\\nthan systematic explorers: linear corridors exemplify a setting where minimal branching but substantial\\ndepth exposes how small per-step errors in state tracking (counting drift, visited-state amnesia, invalid\\ntransitions) compound exponentially. Consequently, test-time scaling alone cannot overcome these structural\\nlimitations without architectural innovations, short horizon error tracking or explicit symbolic grounding.\\n5.1 Current limitations and future work\\nOur study is intentionally narrow; here we outline the main constraints and threats to validity. We focus on\\none-boxlinearcorridors, whichtestlong-horizoncountingandstatemaintenanceratherthanthefulldifficulty\\nof multi-box Sokoban with deadlocks. Thus, the benchmark provides only a lower bound on planning ability.\\nFor evaluation, we use exact-plan validation against a reference generator. Although this is stricter than\\nnecessary in general Sokoban, where multiple optimal plans may exist, it is suitable for corridors; future\\nwork will instead use solver-based verification to handle maps with multiple valid solutions. We also find\\nsensitivity to prompt formatting, especially orientation-related effects such as the many newlines in vertical\\nmaps. Alternative encodings, such as row/column numbered grids or other textual cues, may reduce this\\nissue. Another variability source is model metadata and provider backends: although all calls go through\\none routing layer, backend implementations and model revisions can change over time. We log identifiers\\nand dates, but some instability is inherent in API-based evaluations. Pretraining contamination is another\\nconcern; corridor rotations lower the chance that specific plans were memorized but do not eliminate it.\\n11\\nFinally, corridor tasks have limited external validity, since success or failure may not transfer to richer\\nplanning domains. We treat these settings mainly as a sanity check, with follow-up experiments planned to\\nadd obstacles, branching structures, and deadlocks.\\nSocietal Impact\\nThis paper presents work whose goal is to advance the field of Machine Learning through clearer diagnostics\\nof long-horizon planning. While any benchmark could have indirect downstream effects by steering research\\nagendas, we do not identify specific societal risks unique to this work beyond standard concerns about\\nevaluation misuse. We therefore do not highlight any particular societal impacts at this time.\\nReferences\\nYusra Alkhazraji, Matthias Frorath, Markus Grützner, Malte Helmert, Thomas Liebetraut, Robert\\nMattmüller, Manuela Ortlieb, Jendrik Seipp, Tobias Springenberg, Philip Stahl, et al. Pyperplan.Zenodo,\\n2020.\\nAnthropic. Introducing the model context protocol.https://www.anthropic.com/news/\\nmodel-context-protocol, 2024.\\nDaniel Berend, Dolev Pomeranz, Ronen Rabani, and Ben Raziel. Nonograms: Combinatorial questions and\\nalgorithms.Discrete Applied Mathematics, 169:30–42, 2014.\\nBlai Bonet and Héctor Geffner. Planning as heuristic search.Artificial Intelligence, 129(1-2):5–33, 2001.\\nGregBrockman, VickyCheung, LudwigPettersson, JonasSchneider, JohnSchulman, JieTang, andWojciech\\nZaremba. Openai gym, 2016.\\nZhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao\\nYang, Chen Wei, Xuanke Shi, et al. Has gpt-5 achieved spatial intelligence? an empirical study.arXiv\\npreprint arXiv:2508.13142, 3, 2025.\\nJoseph Culberson. Sokoban is pspace-complete. InProceedings of the International Conference on Fun with\\nAlgorithm, pp. 65–76, June 1998.\\nHourui Deng, Hongjie Zhang, Jie Ou, and Chaosheng Feng. Can llm be a good path planner based on prompt\\nengineering? mitigating the hallucination for path planning. InInternational Conference on Intelligent\\nComputing, pp. 3–15. Springer, 2025.\\nMaria Fox and Derek Long. Pddl2. 1: An extension to pddl for expressing temporal planning domains.\\nJournal of artificial intelligence research, 20:61–124, 2003.\\nMalik Ghallab, Dana Nau, and Paolo Traverso.Automated planning and acting. Cambridge University Press,\\n2016.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang,\\nShirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning.\\nNature, 645(8081):633–638, 2025.\\nPatrik Haslum, Nir Lipovetzky, Daniele Magazzeni, Christian Muise, Ronald Brachman, Francesca Rossi,\\nand Peter Stone.An introduction to the planning domain definition language, volume 13. Springer, 2019.\\nMalte Helmert. The fast downward planning system.Journal of Artificial Intelligence Research, 26:191–246,\\n2006.\\nLanxiangHu, MingjiaHuo, YuxuanZhang, HaoyangYu, EricPXing, IonStoica, TajanaRosing, HaojianJin,\\nand Hao Zhang. lmgame-bench: How good are llms at playing games?arXiv preprint arXiv:2505.15146,\\n2025a.\\n12\\nMengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jian-Guang Lou, Qingwei Lin, Ping Luo, and Saravan\\nRajmohan. Agentgen: Enhancing planning abilities for large language model based agent via environment\\nand task generation. InProceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and\\nData Mining V. 1, pp. 496–507, 2025b.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Alek-\\nsander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card.arXiv preprint arXiv:2412.16720,\\n2024.\\nPetr Jarušek and Radek Pelánek. Difficulty rating of sokoban puzzle. InSTAIRS 2010, pp. 140–150. IOS\\nPress, 2010.\\nAlexia Jolicoeur-Martineau. Less is more: Recursive reasoning with tiny networks.arXiv preprint\\narXiv:2510.04871, 2025.\\nSubbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri,\\nLucas Paul Saldyt, and Anil B Murthy. Position: LLMs can’t plan, but can help planning in llm-modulo\\nframeworks. InForty-first International Conference on Machine Learning, 2024.\\nAndrej Karpathy. Tweet: To help explain the weirdness of llm tokenization.https://twitter.com/\\nkarpathy, 2024. Accessed: 2024-10-08.\\nMaurice Kleine. Nonobench – llm nonogram puzzle solving benchmark.https://nonobench.\\nmauricekleine.com/, 2026. Accessed: 2026-01-08.\\nEmanuele La Malfa, Ping Zhu, Samuele Marro, Sara Bernardini, and Michael Wooldridge. An end-to-end\\nplanning framework with agentic llms and pddl.arXiv preprint arXiv:2512.09629, 2025.\\nJeremiah Lowin. Fastmcp: A high-level framework for building model context protocol\\n(mcp) servers, 2024. URLhttps://github.com/jlowin/fastmcp. Software available from\\nhttps://github.com/jlowin/fastmcp.\\nJiahao Lu, Ziwei Xu, and Mohan Kankanhalli. Reasoning llms are wandering solution explorers.arXiv\\npreprint arXiv:2505.20296, 2025.\\nR Thomas McCoy, Shunyu Yao, Dan Friedman, Mathew D Hardy, and Thomas L Griffiths. Embers of\\nautoregression show how large language models are shaped by the problem they are trained to solve.\\nProceedings of the National Academy of Sciences, 121(41):e2322420121, 2024.\\nDrew McDermott et al. The planning domain definition language manual. Technical report, Technical\\nReport 1165, Yale Computer Science, 1998.(CVC Report 98-003), 1998.\\nElliot Meyerson, Giuseppe Paolo, Roberto Dailey, Hormoz Shahrzad, Olivier Francon, Conor F Hayes, Xin\\nQiu, Babak Hodjat, and Risto Miikkulainen. Solving a million-step llm task with zero errors.arXiv\\npreprint arXiv:2511.09030, 2025.\\nAndrea Micheli, Arthur Bit-Monnot, Gabriele Röger, Enrico Scala, Alessandro Valentini, Luca Framba,\\nAlberto Rovetta, Alessandro Trapasso, Luigi Bonassi, Alfonso Emilio Gerevini, et al. Unified planning:\\nModeling, manipulating and solving ai planning problems in python.SoftwareX, 29:102012, 2025.\\nYoshio Murase, Hitoshi Matsubara, and Yuzuru Hiraga. Automatic making of sokoban problems. InPacific\\nRim International Conference on Artificial Intelligence, pp. 592–600. Springer, 1996.\\nOpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URLhttps://arxiv.org/abs/2508.10925.\\nOpenAI. Gpt-5 technical report.https://openai.com/index/introducing-gpt-5/, 2026.\\nJames Oswald, Kavitha Srinivas, Harsha Kokel, Junkyu Lee, Michael Katz, and Shirin Sohrabi. Large lan-\\nguage models as planning domain generators. InProceedings of the International Conference on Automated\\nPlanning and Scheduling, volume 34, pp. 423–431, 2024.\\n13\\nVishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior\\nHoresh, Francesco Fabiano, and Andrea Loreggia. Understanding the capabilities of large language models\\nfor automated planning.arXiv preprint arXiv:2305.16151, 2023.\\nStuart J. Russell and Peter Norvig.Artificial Intelligence: A Modern Approach. Pearson Education\\nLimited, Harlow, United Kingdom, 4th global edition, 2021. ISBN 978-1292401133. URLhttp:\\n//aima.cs.berkeley.edu.\\nCharles Schepanowski and Charles Ling. On the limits of innate planning in large language models.arXiv\\npreprint arXiv:2511.21591, 2025.\\nAndrew Shin and Kunitake Kaneko. Large language models lack understanding of character composition of\\nwords. InICML 2024 Workshop on LLMs and Cognition, 2024. URLhttps://openreview.net/forum?\\nid=oP5FXcPAeG.\\nYaron Shoham and Jonathan Schaeffer. The fess algorithm: A feature based approach to single-agent search.\\nIn2020 IEEE Conference on Games (CoG), pp. 96–103. IEEE, 2020.\\nParshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad Farajtabar.\\nThe illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of\\nproblem complexity.CoRR, abs/2506.06941, June 2025. URLhttps://arxiv.org/abs/2506.06941.\\nTomSilver, VarunHariprasad, ReeceSShuttleworth, NishanthKumar, TomásLozano-Pérez, andLesliePack\\nKaelbling. PDDL planning with pretrained large language models. InNeurIPS 2022 foundation models\\nfor decision making workshop, 2022.\\nJohn Slaney and Sylvie Thiébaux. Blocks world revisited.Artificial Intelligence, 125(1-2):119–153, 2001.\\nZafir Stojanovski. Wordgame bench.https://wordgamebench.github.io, 2024.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew\\nWen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: A survey on efficient reasoning for large\\nlanguage models.Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URLhttps:\\n//openreview.net/forum?id=HvoG8SxggZ.\\nAyal Taitler, Ron Alford, Joan Espasa, Gregor Behnke, Daniel Fišer, Michael Gimelfarb, Florian Pommeren-\\ning, Scott Sanner, Enrico Scala, Dominik Schreiber, et al. The 2023 international planning competition,\\n2024.\\nMohammad Taufeeque, Philip Quirke, Maximilian Li, Chris Cundy, Aaron David Tucker, Adam Gleave,\\nand Adrià Garriga-Alonso. Planning in a recurrent neural network that plays sokoban.arXiv preprint\\narXiv:2407.15421, 2024.\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\\nChenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms.arXiv\\npreprint arXiv:2501.12599, 2025.\\nOmri Uzan and Yuval Pinter. Charbench: Evaluating the role of tokenization in character-level tasks.arXiv\\npreprint arXiv:2508.02591, 2025.\\nKarthik Valmeekam, Oren Etzioni, Kartik Talamadupula, and Subbarao Srivastava. Planbench: Evaluating\\nlarge language models on planning benchmarks.arXiv preprint arXiv:2206.10498, 2022.\\nKarthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.\\nPlanbench: anextensiblebenchmarkforevaluatinglargelanguagemodelsonplanningandreasoningabout\\nchange. InProceedings of the 37th International Conference on Neural Information Processing Systems,\\nNIPS ’23, Red Hook, NY, USA, 2023a. Curran Associates Inc.\\n14\\nKarthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning\\nabilities of large language models-a critical investigation.Advances in Neural Information Processing\\nSystems, 36:75993–76005, 2023b.\\nKarthik Valmeekam, Kaya Stechly, Atharva Gundawar, and Subbarao Kambhampati. A systematic evalua-\\ntion of the planning and scheduling abilities of the reasoning model o1.Transactions on Machine Learning\\nResearch, 2025. ISSN 2835-8856. URLhttps://openreview.net/forum?id=FkKBxp0FhR.\\nHui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, and Fei Liu. Plangenllms: A modern survey of\\nllm planning capabilities.arXiv preprint arXiv:2502.11221, 2025.\\nNan Xu and Xuezhe Ma. Llm the genius paradox: A linguistic and math expert’s struggle with simple word-\\nbased counting problems. InProceedings of the 2025 Conference of the Nations of the Americas Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),\\npp. 3344–3370, 2025.\\nXuezhi Zhai et al. Planbench: Benchmarking planning capabilities of large language models.arXiv preprint\\narXiv:2502.12345, 2025.\\nChunhui Zhang, Yiren Jian, Zhongyu Ouyang, and Soroush Vosoughi. Working memory identifies reasoning\\nlimits in language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.),Proceedings\\nof the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 16896–16922, Mi-\\nami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.\\nemnlp-main.938. URLhttps://aclanthology.org/2024.emnlp-main.938/.\\nZhenyu Zhang, Tianyi Chen, Weiran Xu, Alex Pentland, and Jiaxin Pei. Recap: Recursive context-aware\\nreasoning and planning for large language model agents.arXiv preprint arXiv:2510.23822, 2025.\\nA Prompts for 1-shot Inference Settings\\nIn this section we report the detailed prompts that we have used throughout our experiments with reasoning\\nmodels alone. Prompts are direct, no Chain of Thought elicited as it is known that it may hamper internal\\nreasoning on LRMs. A simple solved problem is provided as the 1-shot example.\\n15\\nSystem Prompt\\nYou are an a s s i s t a n t that helps in solving a ssi gn ed Sokoban games .\\nYour task is to examine the pr ov ide d Sokoban problem and find a s ol uti on .\\nAll pr ov ide d Sokoban p rob le ms are a ss ign ed in form of ASCII maps .\\nThe mapping is the f o l l o w i n g :\\n‘‘‘\\n@ - Player\\n+ - Player on Goal\\n$ - Box\\n* - Box on Goal\\n. - Goal ( Empty )\\n# - Wall Brick\\n‘‘‘\\nThe game is solved when the box is pushed into the goal position , hence when the pos it ion of ‘$ ‘ c o i n c i d e s with the po sit io n of ‘. ‘.\\nThe player can move in all the empty spaces of the ASCII map while r e s p e c t i n g the walls .\\nWhen the player is adj ac ent to a box , the player can push the box into an ad jac en t empty space .\\nAfter pushing a box , the new po sit io n of the agent will be the po si ti on of the box before the push .\\nThe player cannot pull the box , only push it .\\nThe actions you can perform in the game are :\\n‘‘‘\\nL - Move Left\\nR - Move Right\\nU - Move Up\\nD - Move Down\\n‘‘‘\\nAll pr ov ide d p rob le ms CAN be solved .\\nYou must give your so lu tio n in form of a s equ en ce of allowed actions , s e p a r a t e d by commas .\\nYou must give only the s equ en ce of actions , without any a d d i t i o n a l text or e x p l a n a t i o n .\\nYou must enclose your sol ut ion inside the tags < plan > </ plan >.\\nThe f o l l o w i n g is an example of a Sokoban problem and its so lu tio n :\\nProblem :\\n#####\\n# @ ##\\n## $ ##\\n# #\\n##. ##\\n## #\\n###\\nSo lu tio n :\\n< plan >\\nR ,R ,D , D\\n</ plan >\\nUser Prompt\\nHere is the Sokoban problem to solve , e nc los ed in triple b ac kti cs :\\n‘‘‘\\n{{ s o k o b a n _ m a p }}\\n‘‘‘\\nB Prompts for LLM-Modulo Settings\\nIn this section, we show the system prompt we used for the experiments on LLM-Modulo settings. The user\\nprompt remains the same as shown in Appendix A. The system prompt includes the human-designed PDDL\\ndomain of a typical Sokoban game (https://verificationglasses.wordpress.com/2021/01/02/sokoban-pddl).\\nB.1 System Prompt\\nHere are the system prompts and the PDDL domain being used for the experiments in LLM-Modulo settings.\\nThe model is just required to generate the PDDL problem to be sent to the solver.\\n16\\nSystem Prompt\\nYou are an a s s i s t a n t that helps in solving a ssi gn ed Sokoban games .\\nAll pr ov ide d Sokoban p rob le ms are a ss ign ed in form of ASCII maps and CAN be solved .\\nThe mapping is the f o l l o w i n g :\\n\\\\ ‘\\\\ ‘\\\\ ‘\\n@ --- Player\\n+ --- Player on Goal\\n\\\\ $ --- Box\\n* --- Box on Goal\\n. --- Goal ( Empty )\\n\\\\# --- Wall Brick\\n\\\\ ‘\\\\ ‘\\\\ ‘\\nGiven the PDDL domain of a generic Sokoban game , your task is to gen er ate a valid PDDL problem r e p r e s e n t a t i o n of the pr ovi de d ASCII Sokoban\\nproblem .\\nOnce you ge ner at e the PDDL problem , your final goal is to find a plan that solves the problem .\\nYou have access to a set of tools to help you achieve your goal .\\nAlways use the solve \\\\ _pr ob le m tool to solve the problem , do not try to solve it you rs elf .\\nIf you encur in any error while solving a problem with the tool , try to fix it and call the tool again .\\nRetry up to 3 times at maximum if needed .\\nHere is the PDDL Sokoban domain , e ncl os ed in triple ba ck tic s :\\n\\\\ ‘\\\\ ‘\\\\ ‘\\n\\\\{\\\\{ PDDL \\\\ _domain \\\\}\\\\}\\n\\\\ ‘\\\\ ‘\\\\ ‘\\nI M P O R T A N T :\\nYour final answer must contain both the the PDDL problem and the sol ut io n to the problem without any a d d i t i o n a l text or e x p l a n a t i o n .\\nYou must s e p a r a t e l y enclose the PDDL problem inside the tags < problem > </ problem > , and the s olu ti on inside the tags < plan > </ plan >.\\nIf , after the third attempt , you are unable to get a so lut io n from the solver , provide the error message you r ece iv ed from the tool inside\\nthe < plan > </ plan > tags .\\nIf at the end of your process the solve \\\\ _ pr obl em tool gets called without errors and returns a solution , write < solver > True </ solver > ,\\no t h e r w i s e write < solver > False </ solver >.\\nExample output :\\n\\\\ ‘\\\\ ‘\\\\ ‘ yaml\\nproblem : < problem > PDDL problem here </ problem >\\nplan : < plan > PDDL plan from solver here </ plan >\\nsolver : < solver > Boolean ch ec kin g whether solve \\\\ _ pr obl em tool was called successfully </ solver >\\n\\\\ ‘\\\\ ‘\\\\ ‘\\nB.2 PDDL Domain\\nHere the human authored PDDL domain used in the above system prompt is reported for completeness.\\nPDDL Domain\\n( define ( domain sokoban )\\n(: p r e d i c a t e s ( wall ? x ? y ) ( box ? x ? y ) ( at ? x ? y ) ( inc ? p ? pp ) ( dec ? pp ? p ) )\\n(: action move - up\\n: p a r a m e t e r s (? x ? y ? xn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? xn ? y ) ) ( not ( box ? xn ? y ) ) ( dec ? x ? xn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? xn ? y ) )\\n)\\n(: action move - down\\n: p a r a m e t e r s (? x ? y ? xn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? xn ? y ) ) ( not ( box ? xn ? y ) ) ( inc ? x ? xn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? xn ? y ) )\\n)\\n(: action move - right\\n: p a r a m e t e r s (? x ? y ? yn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? x ? yn ) ) ( not ( box ? x ? yn ) ) ( inc ? y ? yn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? x ? yn ) )\\n)\\n(: action move - left\\n: p a r a m e t e r s (? x ? y ? yn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? x ? yn ) ) ( not ( box ? x ? yn ) ) ( dec ? y ? yn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? x ? yn ) )\\n)\\n(: action push - up\\n: p a r a m e t e r s (? x ? y ? xn ? xnn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? xn ? y ) ) ( box ? xn ? y ) ( dec ? x ? xn ) ( not ( wall ? xnn ? y ) ) ( not ( box ? xnn ? y ) ) ( dec ? xn ? xnn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? xn ? y ) ( not ( box ? xn ? y ) ) ( box ? xnn ? y ) )\\n)\\n(: action push - down\\n: p a r a m e t e r s (? x ? y ? xn ? xnn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? xn ? y ) ) ( box ? xn ? y ) ( inc ? x ? xn ) ( not ( wall ? xnn ? y ) ) ( not ( box ? xnn ? y ) ) ( inc ? xn ? xnn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? xn ? y ) ( not ( box ? xn ? y ) ) ( box ? xnn ? y ) )\\n)\\n(: action push - right\\n: p a r a m e t e r s (? x ? y ? yn ? ynn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? x ? yn ) ) ( box ? x ? yn ) ( inc ? y ? yn ) ( not ( wall ? x ? ynn ) ) ( not ( box ? x ? ynn ) ) ( inc ? yn ? ynn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? x ? yn ) ( not ( box ? x ? yn ) ) ( box ? x ? ynn ) )\\n)\\n(: action push - left\\n: p a r a m e t e r s (? x ? y ? yn ? ynn )\\n: p r e c o n d i t i o n ( and ( at ? x ? y ) ( not ( wall ? x ? yn ) ) ( box ? x ? yn ) ( dec ? y ? yn ) ( not ( wall ? x ? ynn ) ) ( not ( box ? x ? ynn ) ) ( dec ? yn ? ynn ) )\\n: effect ( and ( not ( at ? x ? y ) ) ( at ? x ? yn ) ( not ( box ? x ? yn ) ) ( box ? x ? ynn ) )\\n)\\n)\\n17\\nC LLM-Modulo: Map Rotations\\nIn this section we show the results of the LLM-modulo setting in all map rotations separately. Accuracies\\nare just averaged over the four experiment trials.\\n20 40 60 80 100\\nCorridor Length \\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAverage Accuracy\\nrotation: 0\\n20 40 60 80 100\\nCorridor Length \\nrotation: 90\\n20 40 60 80 100\\nCorridor Length \\nrotation: 180\\n20 40 60 80 100\\nCorridor Length \\nrotation: 270\\nFigure 8: GPT-5 mini accuracies in LLM-modulo setting, averaged over four experiment trials on each\\nSokoban corridor rotation.\\n18\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b6c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_row(pdf_text, QUESTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4352db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The focus of the research paper is to systematically evaluate the long-horizon planning and reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs) using a simplified version of Sokoban puzzles. The authors aim to investigate how well these models can perform long-horizon planning tasks, particularly when the complexity of the environment is minimized to isolate the planning aspect from state persistence. The study reveals that LRMs exhibit a consistent degradation in planning performance when the number of required moves exceeds 25, indicating inherent limitations in their forward planning capacity. The paper also explores the potential for modest improvements in performance through the integration of Planning Domain Definition Language (PDDL) tools, highlighting the architectural constraints of LRMs in handling long-term action representation and sequential logic.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7b2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Milan2025DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
